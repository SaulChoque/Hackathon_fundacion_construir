import os
import json
<<<<<<< HEAD
=======
from transformers import AutoModelForCausalLM, AutoTokenizer
>>>>>>> 59d6e9eee3775cc73dc8258ccfff38366b23a8b7
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

<<<<<<< HEAD
def cargar_datos_enriquecidos(path_json):
    with open(path_json, "r", encoding="utf-8") as f:
        return json.load(f)

def generar_prompt(datos, tema):
    contexto = "\n\n".join(
        f"Fuente: {item['fuente']}\nT√≠tulo: {item['titulo']}\nContenido: {item['resumen'][:1500]}..."
        for item in datos
    )

    crisis_detectada = any("crisis" in d["resumen"].lower() for d in datos)
    locutor_intro = "crisis econ√≥mica" if crisis_detectada else "transici√≥n pol√≠tica"

    prompt = f"""Eres un analista pol√≠tico experto en elecciones bolivianas. Genera un guion para un video documental de 7-10 minutos sobre:
=======

def procesar_resultados(archivo_json):
    """formato JSON con:
    - Filtro de calidad de contenido
    - Priorizaci√≥n de fuentes clave
    - Normalizaci√≥n de texto"""
    
    with open(archivo_json, 'r', encoding='utf-8') as f:
        datos = json.load(f)
    
    resultados = []
    fuentes_prioritarias = ["infobae.com", "lapatria.bo", "lostiempos.com"]
    
    for item in datos:
        # Filtros b√°sicos
        if not item.get("resumen") or len(item["resumen"]) < 150:
            continue
            
        # Normalizaci√≥n
        contenido = item["resumen"]
        if "Fotograf√≠a de archivo" in contenido:
            contenido = contenido.split("Fotograf√≠a de archivo")[0]
        
        # Priorizar fuentes confiables
        prioridad = 2 if any(fp in item["url"] for fp in fuentes_prioritarias) else 1
        
        resultados.append({
            "titulo": item["titulo"],
            "fuente": item["fuente"],
            "contenido": contenido[:2000],  # Limitar tama√±o
            "prioridad": prioridad,
            "fecha": item.get("resumen", "").split(" ")[0]  # Extrae fecha del resumen
        })
    
    # Ordenar por prioridad y longitud de contenido
    return sorted(
        resultados,
        key=lambda x: (-x["prioridad"], -len(x["contenido"]))
    

# ==============================
# 2. GENERADOR DE SHORTS OPTIMIZADO
# ==============================
def generar_shorts(datos, tema):
    """Genera guiones ultra-concisos para YouTube Shorts con:
    - Estructura cronometrada exacta
    - Datos espec√≠ficos de tus fuentes
    - Adaptaci√≥n al formato vertical"""
    
    # Preparar contexto para IA (top 3 fuentes)
    contexto = "\n\n".join(
        f"üìå {item['titulo']} ({item['fuente']}, {item.get('fecha', 's/f')}):\n"
        f"{item['contenido'][:300]}..."
        for item in datos[:3]
    )
>>>>>>> 59d6e9eee3775cc73dc8258ccfff38366b23a8b7

    prompt = f"""**Instrucciones exactas para YouTube Short (30-60 segundos):**

**Tema:** {tema}
**Formato OBLIGATORIO:**
[0:00-0:04] üé£ HOOK: Afirmaci√≥n impactante (usar n√∫meros/controversia)
[0:04-0:12] üéØ CONTEXTO: 1 frase sobre el candidato/elecci√≥n
[0:12-0:30] üí° PROPUESTAS: 2 M√ÅXIMO (con datos concretos)
[0:30-0:50] üî• IMPACTO: 1 consecuencia o dato pol√©mico
[0:50-1:00] ‚ùì CTA: Pregunta para comentarios + emoji

**Fuentes disponibles (usar datos exactos):**
{contexto}

**Ejemplo real basado en fuentes:**
[Hook] "¬°Doria Medina quiere ELIMINAR el SENADO! (Ahorro: $200M/a√±o)"
[Contexto] "Candidato opositor lidera encuestas para 2025"
[Propuestas] 
1. Autonom√≠as econ√≥micas regionales
2. Voto electr√≥nico desde 2026
[Impacto] "Seg√∫n Infobae: 68% apoya pero congreso rechaza"
[CTA] "¬øEs viable este plan? üëá"

<<<<<<< HEAD
2. PROPUESTAS CLAVE (4-5 mins):
   - Econ√≥micas (m√°x 2 propuestas concretas con datos)
   - Pol√≠ticas (reforma electoral, etc.)
   - Sociales (educaci√≥n, salud)
   - Citando fuentes espec√≠ficas cuando corresponda

3. AN√ÅLISIS (2 mins):
   - Posibles impactos de las propuestas
   - Comparaci√≥n breve con otros candidatos
   - Viabilidad pol√≠tica

4. CONCLUSI√ìN (1 min):
   - Resumen ejecutivo
   - Frase de cierre impactante

**Estilo:**
- Formal pero accesible
- Neutral (no tomar partido)
- Usar datos verificables
- Incluir al menos 3 citas textuales de las fuentes

[INICIO DEL GUION]
**T√≠tulo:** "An√°lisis de las propuestas de Samuel Doria Medina para Bolivia 2025"

**[INTRODUCCI√ìN]**
(Video: Im√°genes de campa√±a electoral)
Locutor: "En un contexto de {locutor_intro}, las elecciones bolivianas de 2025 marcar√°n..."
"""
    return prompt

def generar_guion(prompt, hf_token):
    print("üîÑ Cargando modelo y tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", use_auth_token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1",
        use_auth_token=hf_token,
=======
**Guion a generar (seguid el formato):**"""
    
    # Configuraci√≥n para respuestas ultra-cortas
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1",
>>>>>>> 59d6e9eee3775cc73dc8258ccfff38366b23a8b7
        torch_dtype=torch.float16,
        device_map="auto"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
<<<<<<< HEAD

    outputs = model.generate(
        **inputs,
        max_new_tokens=1200,
        temperature=0.6,
=======
    outputs = model.generate(
        **inputs,
        max_new_tokens=300,  # Extremadamente conciso
        temperature=0.7,
>>>>>>> 59d6e9eee3775cc73dc8258ccfff38366b23a8b7
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.2
    )
<<<<<<< HEAD

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def guardar_guion(guion, tema):
    nombre_archivo = "guion_final.md"
    with open(nombre_archivo, "w", encoding="utf-8") as f:
        f.write(f"# Guion Video Documental: {tema}\n\n")
        f.write(guion)
    print(f"‚úÖ Guion guardado en: {nombre_archivo}")

def main():
    tema = "propuestas electorales de Samuel Doria Medina para las elecciones presidenciales de Bolivia 2025"
    path_json = "resultados_mejorados_Webscrapper_example.json"

    if not os.path.exists(path_json):
        print("‚ùå Archivo JSON de entrada no encontrado.")
        return

    print("üì• Cargando datos enriquecidos...")
    datos = cargar_datos_enriquecidos(path_json)
    if not datos:
        print("‚ùå No se encontraron datos v√°lidos.")
        return

    print("‚úçÔ∏è Generando prompt para el modelo...")
    prompt = generar_prompt(datos, tema)

    # Pega aqu√≠ tu token de Hugging Face que tiene acceso al modelo Mistral-7B
    hf_token = "coloca_tu_token_aqu√≠"

    if not hf_token:
        print("‚ùå No se ha definido el token de Hugging Face.")
        return

    print("üß† Generando guion con Mistral-7B...")
    guion = generar_guion(prompt, hf_token)

    print("üíæ Guardando guion en archivo Markdown...")
    guardar_guion(guion, tema)

    print("\nüì¢ EXTRACTO DEL GUION:")
    print(guion[:800] + "...\n")

if __name__ == "__main__":
    main()
=======
    
    guion = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return guion.split("**Guion a generar:**")[-1].strip()

# ==============================
# 3. EJECUCI√ìN PRINCIPAL
# ==============================
if __name__ == "__main__":
    # Configuraci√≥n
    TEMA = "Samuel Doria Medina: Propuestas clave 2025"
    ARCHIVO_JSON = "resultados_mejorados_Webscrapper_example.json"  # Tu archivo actualizado
    
    # Paso 1: Procesar datos
    print("üìä Procesando resultados JSON...")
    datos_procesados = procesar_resultados(ARCHIVO_JSON)
    
    if not datos_procesados:
        print("‚ùå No hay datos v√°lidos para generar el guion")
        exit()
    
    # Paso 2: Generar guion para Shorts
    print("üé¨ Generando guion para YouTube Shorts...")
    guion = generar_shorts(datos_procesados, TEMA)
    
    # Guardar resultado
    with open("shorts_final.txt", "w", encoding="utf-8") as f:
        f.write(f"# GUION SHORT - {TEMA}\n\n")
        f.write(guion)
    
    print("\n‚úÖ ¬°Guion generado con √©xito!")
    print(f"- Archivo guardado: shorts_final.txt")
    print("\n--- EXTRACTO ---")
    print(guion[:500] + "...")
>>>>>>> 59d6e9eee3775cc73dc8258ccfff38366b23a8b7
