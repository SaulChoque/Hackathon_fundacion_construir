import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def cargar_datos_enriquecidos(path_json):
    with open(path_json, "r", encoding="utf-8") as f:
        return json.load(f)

def generar_prompt(datos, tema):
    contexto = "\n\n".join(
        f"Fuente: {item['fuente']}\nT√≠tulo: {item['titulo']}\nContenido: {item['resumen'][:1500]}..."
        for item in datos
    )

    crisis_detectada = any("crisis" in d["resumen"].lower() for d in datos)
    locutor_intro = "crisis econ√≥mica" if crisis_detectada else "transici√≥n pol√≠tica"

    prompt = f"""Eres un analista pol√≠tico experto en elecciones bolivianas. Genera un guion para un video documental de 7-10 minutos sobre:

**Tema central:** {tema}

**Fuentes investigadas:**
{contexto}

**Estructura requerida:**
1. INTRODUCCI√ìN (1 min):
   - Contexto hist√≥rico de las elecciones 2025
   - Presentaci√≥n del candidato
   - Mencionar fuentes consultadas

2. PROPUESTAS CLAVE (4-5 mins):
   - Econ√≥micas (m√°x 2 propuestas concretas con datos)
   - Pol√≠ticas (reforma electoral, etc.)
   - Sociales (educaci√≥n, salud)
   - Citando fuentes espec√≠ficas cuando corresponda

3. AN√ÅLISIS (2 mins):
   - Posibles impactos de las propuestas
   - Comparaci√≥n breve con otros candidatos
   - Viabilidad pol√≠tica

4. CONCLUSI√ìN (1 min):
   - Resumen ejecutivo
   - Frase de cierre impactante

**Estilo:**
- Formal pero accesible
- Neutral (no tomar partido)
- Usar datos verificables
- Incluir al menos 3 citas textuales de las fuentes

[INICIO DEL GUION]
**T√≠tulo:** "An√°lisis de las propuestas de Samuel Doria Medina para Bolivia 2025"

**[INTRODUCCI√ìN]**
(Video: Im√°genes de campa√±a electoral)
Locutor: "En un contexto de {locutor_intro}, las elecciones bolivianas de 2025 marcar√°n..."
"""
    return prompt

def generar_guion(prompt, hf_token):
    print("üîÑ Cargando modelo y tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", use_auth_token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1",
        use_auth_token=hf_token,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=1200,
        temperature=0.6,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.1
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def guardar_guion(guion, tema):
    nombre_archivo = "guion_final.md"
    with open(nombre_archivo, "w", encoding="utf-8") as f:
        f.write(f"# Guion Video Documental: {tema}\n\n")
        f.write(guion)
    print(f"‚úÖ Guion guardado en: {nombre_archivo}")

def main():
    tema = "propuestas electorales de Samuel Doria Medina para las elecciones presidenciales de Bolivia 2025"
    path_json = "resultados_mejorados_Webscrapper_example.json"

    if not os.path.exists(path_json):
        print("‚ùå Archivo JSON de entrada no encontrado.")
        return

    print("üì• Cargando datos enriquecidos...")
    datos = cargar_datos_enriquecidos(path_json)
    if not datos:
        print("‚ùå No se encontraron datos v√°lidos.")
        return

    print("‚úçÔ∏è Generando prompt para el modelo...")
    prompt = generar_prompt(datos, tema)

    # Pega aqu√≠ tu token de Hugging Face que tiene acceso al modelo Mistral-7B
    hf_token = "coloca_tu_token_aqu√≠"

    if not hf_token:
        print("‚ùå No se ha definido el token de Hugging Face.")
        return

    print("üß† Generando guion con Mistral-7B...")
    guion = generar_guion(prompt, hf_token)

    print("üíæ Guardando guion en archivo Markdown...")
    guardar_guion(guion, tema)

    print("\nüì¢ EXTRACTO DEL GUION:")
    print(guion[:800] + "...\n")

if __name__ == "__main__":
    main()
